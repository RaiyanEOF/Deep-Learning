{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# DL Assignment 01\n",
        "\n",
        "**Name:**\n",
        "\n",
        "**Course Email:**  \n",
        "\n",
        "This is a small assignment that connects topics from Module 1, 2, and 3.  \n",
        "\n",
        "## End of Assignment\n",
        "\n",
        "Before submitting:\n",
        "- Run all cells from top to bottom.  \n",
        "- Check that all answer sections are filled.  \n",
        "- Instruction video অনুযায়ী আমাদের দেয়া Colab ফাইলটি থেকে প্রথম একটি Save copy in drive করে নিবা। এরপর Google colab এর মধ্যে কোডগুলো করবে এবং সেই ফাইলটি ‘Anyone with the link’ & ‘View’ Access দিয়ে ফাইলটির Shareble Link টি সাবমিট করবে।"
      ],
      "metadata": {
        "id": "JcVoA-oxVACn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 01: [ Marks 10 ]\n",
        "\n",
        "What is a perceptron?\n",
        "Explain its three main components and their roles.\n"
      ],
      "metadata": {
        "id": "d18z86IcVO3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write** Answer 01:\n",
        "\n",
        "**A perceptron is the simplest type of artificial neural network and is used for binary classification, meaning it predicts one of two possible outputs such as 0 or 1. It is inspired by the human brain’s neuron, as it takes multiple inputs, processes them, and produces a single output.**\n",
        "\n",
        "**Three Main Components of a Perceptron:**\n",
        "\n",
        "**1. Weights:**\n",
        "**Weights are values assigned to each input to show their importance. Each input is multiplied by its weight, and higher weights have a greater influence on the final result.**\n",
        "\n",
        "**2. Bias:**\n",
        "**Bias is an extra value added to the weighted sum. It helps shift the decision boundary so the model can classify data more accurately.**\n",
        "\n",
        "**3. Activation Function:**\n",
        "**The activation function decides the final output. A perceptron usually uses a step function that produces 0 or 1 depending on whether the result crosses a certain threshold.**"
      ],
      "metadata": {
        "id": "uEW6ibGfVZW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 02: [ Marks 10 ]\n",
        "\n",
        "What is a decision boundary in a perceptron?\n",
        "Explain how the equation\n",
        "w₁x₁ + w₂x₂ + b = 0\n",
        "represents a decision boundary geometrically.\n"
      ],
      "metadata": {
        "id": "_tPnXmkMVi8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Answer 02:\n",
        "**A decision boundary in a perceptron is a line or a plane that separates data points into different classes. It is the boundary where the perceptron is uncertain about the class, meaning the output changes from one category to another.**\n",
        "\n",
        "**The equation w1x1 + w2x2 + b = 0 represents this boundary geometrically. Here,x1, x2 are the input feature, w1,w2 are their weights and, b is the bias. When the equation equals zero, the points lie exactly on the decision boundary.**\n",
        "\n",
        "**If w1x1 + w2x2 + b > 0, the perceptron classifies the point into one class.**\n",
        "\n",
        "**If w1x1 + w2x2 + b < 0, it classifies the point into the other class.**\n",
        "\n",
        "**Geometrically, this equation forms a straight line on a graph that divides the feature space into two halves. The weights control the slope of the line, while the bias shifts the line left, right, up, or down. This allows the perceptron to separate the data based on their features.**"
      ],
      "metadata": {
        "id": "oTQ9yUrlVk4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 03: [ Marks 15 ]\n",
        "\n",
        "Why is the perceptron called a linear classifier?\n",
        "Explain your answer using the concept of linear separability and logical gate examples such as AND, OR, and XOR.\n"
      ],
      "metadata": {
        "id": "YiUWig4VVnXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Answer 03:\n",
        "**A perceptron is called a linear classifier because it classifies data by creating a linear decision boundary, such as a straight line in two dimensions or a plane in higher dimensions. It uses a linear equation w1x1 + w2x2 + b to separate data points into different classes. Since the boundary is linear, the perceptron can only correctly classify data that is linearly separable.**\n",
        "\n",
        "**Linear separability means that data points from different classes can be separated using a single straight line. If the data is linearly separable, the perceptron can learn the pattern and make accurate predictions. However, if the data cannot be divided by one straight line, the perceptron will fail to classify it properly.**\n",
        "\n",
        "**Examples using Logical Gates:**\n",
        "\n",
        "**AND -> The output is 1 only when both inputs are 1. These points can be separated from the others with a single straight line, so a perceptron can model the AND gate successfully.**\n",
        "\n",
        "**OR -> The output is 1 when at least one input is 1. This dataset is also linearly separable, allowing the perceptron to classify it correctly.**\n",
        "\n",
        "**XOR -> The output is 1 when the inputs are different. These points are not linearly separable because no single straight line can divide them. Therefore, a single perceptron cannot solve the XOR problem.**"
      ],
      "metadata": {
        "id": "kTpWEibjaGmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 04: [ Marks 15 ]\n",
        "\n",
        "Write the perceptron weight update rule and explain it intuitively.\n",
        "Why are the weights updated only when the prediction is incorrect?"
      ],
      "metadata": {
        "id": "Vh-CVlcIZMdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Answer 04:\n",
        "**The perceptron weight update rule is used to adjust the weights and bias so that the perceptron can improve its predictions during training. The update is performed only when the model makes an incorrect prediction.**\n",
        "**Weight Update Formula : wi = wi + n(y-y')xi & b = b +n(y-y')**\n",
        "**Here wi is the weight, n is the learning rate, y is the actual output and y' is the predicted output, xi is the input features and finally b is the bias.**\n",
        "\n",
        "**The perceptron learns by correcting its mistakes. When the predicted output is wrong, the weights are adjusted to shift the decision boundary in the correct direction. This helps the model make better predictions for similar inputs in the future.**\n",
        "\n",
        "**If the prediction is correct, then y-y'= 0 so no update is needed. Updating the weights unnecessarily could disturb the correct decision boundary and reduce accuracy. Therefore, the perceptron updates weights only when it makes an error, allowing it to gradually find the best boundary to separate the data.**"
      ],
      "metadata": {
        "id": "rfdsvUWfaJJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 05: [ Marks 10 ]\n",
        "\n",
        "Why does a single-layer perceptron fail to solve the XOR problem?\n",
        "What does this limitation tell us about the need for multilayer neural networks?"
      ],
      "metadata": {
        "id": "WIr6YmxaZNli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Answer 05:\n",
        "\n",
        "**A single-layer perceptron fails to solve the XOR problem because the XOR dataset is not linearly separable. A perceptron can only create a straight-line decision boundary, but in the XOR gate, the outputs are 1 when the inputs are different (0,1) and (1,0), and 0 when the inputs are the same (0,0) and (1,1). When these points are plotted on a graph, no single straight line can separate the 1s from the 0s, a non-linear boundary is required. Since a single-layer perceptron cannot form non-linear boundaries, it cannot classify XOR correctly. This limitation shows the need for multilayer neural networks, which include hidden layers and activation functions that allow the model to create complex, non-linear decision boundaries. As a result, multilayer networks can solve problems like XOR and learn more complicated patterns, making them more powerful than single-layer perceptrons.**"
      ],
      "metadata": {
        "id": "RVJMptexaKjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 06: [ Marks 20 ]\n",
        "\n",
        "What is meant by the “perceptron Learning Rule” or weight adjustment method?\n",
        "Why can adjusting one weight affect the classification of other points?"
      ],
      "metadata": {
        "id": "t7wLPZTPZOnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Answer 06:\n",
        "\n",
        "**The Perceptron Learning Rule, also known as the weight adjustment method, is the process by which a perceptron updates its weights and bias to improve classification accuracy. When the perceptron makes a wrong prediction, the weights are adjusted using the formula wi = wi + n(y-y')xi where, wi is the weight, n is the learning rate, y is the actual output and y' is the predicted output, xi is the input features. This adjustment shifts the decision boundary so that the incorrectly classified point can be placed on the correct side. Adjusting one weight can affect the classification of other points because the weights determine the position and angle of the decision boundary. When a weight changes, the boundary moves or rotates, which may cause some previously correct points to become incorrect or vice versa. Therefore, the perceptron continuously updates its weights until it finds a boundary that best separates the data.**"
      ],
      "metadata": {
        "id": "qNmHw4tnaMFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 07: [ Marks 20 ]\n",
        "\n",
        "What are the main limitations of a single-layer perceptron?\n",
        "How does the idea of using more than one layer (MLP) help overcome these limitations at a high level accoriding to you?\n",
        "\n"
      ],
      "metadata": {
        "id": "zZMx7oNaZPqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Answer 07:\n",
        "**The single-layer perceptron has several important limitations. First, it can only solve linearly separable problems, meaning it can separate data using a straight line or plane. Because of this, it fails to handle more complex datasets such as the XOR problem. Second, it has limited learning capability since it does not have hidden layers to capture complex patterns or relationships between features. Third, it cannot create non-linear decision boundaries, which are often required in real-world problems like image recognition or speech processing.**\n",
        "\n",
        "**The idea of using more than one layer, known as a Multi-Layer Perceptron, helps overcome these limitations. MLPs include one or more hidden layers placed between the input and output layers. These hidden layers, along with activation functions, allow the network to learn complex and non-linear patterns. Instead of relying on a single straight boundary, multilayer networks can combine multiple boundaries to form curved or more flexible decision regions.**"
      ],
      "metadata": {
        "id": "L3cixQ3gaN0J"
      }
    }
  ]
}